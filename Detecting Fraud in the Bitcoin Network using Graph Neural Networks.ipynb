{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HYv235Lx2gQ0","executionInfo":{"status":"ok","timestamp":1685981091326,"user_tz":-120,"elapsed":57229,"user":{"displayName":"JAVIER SANCHEZ-PASCUAL CASTROMONTE","userId":"12135981990345752370"}},"outputId":"c336a32a-bf63-44b3-88ea-5fabdf6199b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.0.1+cu118\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting geometric-smote\n","  Downloading geometric_smote-0.2.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.10/dist-packages (from geometric-smote) (1.10.1)\n","Requirement already satisfied: numpy>=1.1 in /usr/local/lib/python3.10/dist-packages (from geometric-smote) (1.22.4)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.10/dist-packages (from geometric-smote) (1.2.2)\n","Requirement already satisfied: imbalanced-learn>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from geometric-smote) (0.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.4.3->geometric-smote) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn>=0.4.3->geometric-smote) (3.1.0)\n","Installing collected packages: geometric-smote\n","Successfully installed geometric-smote-0.2.0\n"]}],"source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install -U geometric-smote"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"82a68f2d","executionInfo":{"status":"ok","timestamp":1685981740536,"user_tz":-120,"elapsed":251,"user":{"displayName":"JAVIER SANCHEZ-PASCUAL CASTROMONTE","userId":"12135981990345752370"}}},"outputs":[],"source":["import os\n","import copy\n","import torch\n","import warnings\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample \n","\n","from torch_geometric.utils import to_networkx\n","from torch_geometric.data import Data, DataLoader\n","\n","import torch.nn.functional as F\n","from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n","import torch.nn as nn\n","from torch_geometric.nn import *\n","\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":281,"status":"ok","timestamp":1685981741306,"user":{"displayName":"JAVIER SANCHEZ-PASCUAL CASTROMONTE","userId":"12135981990345752370"},"user_tz":-120},"id":"40321f64","outputId":"4a642ccf-1528-4577-d2d4-06bab100148e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n"]}],"source":["class Config:\n","    seed = 0\n","    learning_rate = 0.001\n","    weight_decay = 1e-5\n","    input_dim = 165\n","    output_dim = 1\n","    hidden_size = 128\n","    num_layers = 3\n","    num_epochs = 100\n","    checkpoints_dir = './models/elliptic_gnn'\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    \n","print(\"Using device:\", Config.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LF7xkO2zaAoI"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa155275"},"outputs":[],"source":["# DATA LOADING/PREPARATION\n","df_features = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/elliptic_bitcoin_dataset/elliptic_txs_features.csv', header=None)\n","df_edges = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/elliptic_bitcoin_dataset/elliptic_txs_edgelist.csv\")\n","df_classes =  pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/elliptic_bitcoin_dataset/elliptic_txs_classes.csv\")\n","df_classes['class'] = df_classes['class'].map({'unknown': 2, '1': 1, '2': 0})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KP4bdmcSP3Qh"},"outputs":[],"source":["print(df_features.shape)\n","df_features.head()\n","#df_features[df_features[0].isna()].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hn8sdo5HV338"},"outputs":[],"source":["df_edges.head()\n","#df_edges[df_edges['txId2'].isna()].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DqSjJmCuNv4H"},"outputs":[],"source":["# Hay dos problemas: \n","# - Está desbalanceado y hay que oversamplear la clase 1 para que iguale las muestras de la clase 0.\n","# - La clase 2 es desconocida por lo que solo aporta ruido. Hay que eliminar todo rastro de esa clase antes de oversampling\n","print(df_classes['class'].value_counts())\n","#df_classes[df_classes['txId'].isna()].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LGJfNCe2ODoH"},"outputs":[],"source":["# PASO 1: Eliminar todo rastro de la clase desconocida '2' para evitar ruido y poder oversamplear\n","# 1.1 Quitamos los nodos con clase 2 del df_classes (fácil)\n","# Quitamos los nodos de la clase desconocida\n","df_classes2 = df_classes[df_classes['class'] != 2 ] \n","print(df_classes2.shape)\n","print(df_classes2['class'].value_counts())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAOLg9nNPj9X"},"outputs":[],"source":["# 1.2 Quitamos los nodos con clase 2 del df_features (fácil). Hacemos join con df_classes para saber la clase y suprimirla\n","print(\"df_features antes: \", df_features.shape)\n","df_features2 = pd.merge(df_features,df_classes2,how='inner', left_on=[0], right_on=['txId']).drop(['txId'], axis=1)\n","print(\"df_features despues: \", df_features2.shape)\n","df_features2.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4W_LE6kTTDi"},"outputs":[],"source":["# 1.3 Quitamos todas las aristas que tengan algo que ver con la clase 2 (menos fácil). Lo hacemos mediante 2 outer joins (uno contra los nodos origen y otro contra los nodos destino) de la clase desconocida \n","# Quitamos los edges donde están involucrados nodos de la clase desconocida\n","print(\"df_edges antes: \", df_edges.shape)\n","df_classes_unknown = df_classes.loc[df_classes['class'] == 2, 'txId']\n","print(\"Número de nodos pertenecientes a la clase desconocida: \", df_classes_unknown.shape)\n","df_edges2 = pd.merge(df_classes_unknown, df_edges, indicator=True, how = 'outer', left_on=['txId'], right_on=['txId1']).query('_merge==\"right_only\"').drop(['txId', '_merge'], axis=1)\n","print(\"df_edges sin clase 2 en los nodos origen: \", df_edges2.shape)\n","df_edges2 = pd.merge(df_classes_unknown, df_edges2, indicator=True, how = 'outer', left_on=['txId'], right_on=['txId2']).query('_merge==\"right_only\"').drop(['txId', '_merge'], axis=1)\n","print(\"df_edges sin clase 2 en los nodos origen ni destino: \", df_edges2.shape)\n","df_edges2.head()\n","df_edges2 = df_edges2.astype(int)\n","# Vemos que una vez se ha eliminado todo rastro de la clase 2, nos encontramos con que hay menos aristas que nodos de las clase 1 y 2 juntas: 36624 vs 46564"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7EAc4PABifD"},"outputs":[],"source":["# PASO 2: Una vez se ha \"limpiado\" la clase 2, es necesario fusionar los dfs para tener una única tabla que será la entrada del SMUTE (oversampler)\n","# 2.1 Añadimos al df_features la columna de clase (fácil). Debería resultar en un mismo número de filas (df_features2)\n","\n","# 2.2 Añadimos la columna 'destino' al df_features (alias txId2). \n","# El resultado será un aumento en el número de filas ya que un nodo puede ser origen de varias aristas.\n","# Convertimos los NaN a 0 para no tener problemas. Los NaN solo aparecerán en la columna 'destino' porque hay nodos que no son origen de ninguna arista\n","df_join = pd.merge(df_features2, df_edges2, how = 'left', left_on=[0], right_on=['txId1']).drop('txId1', axis=1).drop_duplicates()\n","df_join['txId2'].fillna(0, inplace=True)\n","print(\"El df resultado tiene la siguiente forma: \", df_join.shape)\n","print(\"La distribución de clase en formato tabular seria:\\n\",df_join['class'].value_counts())\n","df_join.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XgF5lPb2Xjw2"},"outputs":[],"source":["# PASO 3. Una vez tenemos los datos en forma tabular, los dividimos para la entrada en el oversampler:\n","X = df_join.rename(columns={'txId2':len(df_join.columns)-2}).drop('class', axis=1)\n","y = df_join['class']\n","print(\"La entrada X tendría la siguiente forma: \", X.shape)\n","print(\"La entrada y tendría la siguiente forma: \", y.shape)\n","print(\"Distribución de y:\\n\", y.value_counts())\n","X.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v2Mmm1ClZAGX"},"outputs":[],"source":["# PASO 4. Realizamos el OVERSAMPLING.\n","from imblearn.over_sampling import SMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.pipeline import Pipeline\n","import pandas as pd\n","from sklearn.impute import SimpleImputer\n","# Aumentamos la clase menor con G-SMOTE\n","from gsmote import GeometricSMOTE\n","\n","ss_over = 0.99\n","ss_under = 0.5\n","\n","# Definir la estrategia de sobremuestreo y submuestreo\n","over = SMOTE(sampling_strategy=ss_over)\n","#over = GeometricSMOTE(k_neighbors=2, selection_strategy='combined', random_state=5)\n","under = RandomUnderSampler(sampling_strategy=ss_under)\n","\n","# Combinar ambas estrategias en un pipeline , ('o', over), ('u', under)\n","steps = [ ('u', under), ('o', over)]  \n","pipeline = Pipeline(steps=steps)\n","\n","X_res, y_res = pipeline.fit_resample(X, y)\n","\n","print(\"Resultado oversampling tabular:\\n\", y_res.value_counts())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nDhEmQ8ehxW4"},"outputs":[],"source":["# PASO 5. VOLVER AL FORMATO GRAFO\n","# 5.1 JUNTAMOS dfs para incluir el campo class\n","print(X_res.shape)\n","X_res['class'] = y_res\n","X_res[167] = X_res[167].astype(int)\n","X_res = X_res.drop_duplicates(subset=[0, 'class'], keep=False)\n","print(X_res.shape)\n","X_res.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2sc6wKMNi0UZ"},"outputs":[],"source":["# 5.2 Reconstruimos df de classes\n","# 1. Montamos df de classes cogiendo tanto la columna \"origen\" (txId1 alias 0) como la columna \"destino\" (txId2 alias 167). Eliminamos todas las apariciones de los nodos con más\n","df_c_1 = X_res[[0]].drop_duplicates().rename(columns={0:'txId'}).sort_values('txId').reset_index(drop=True)\n","df_c_2 = X_res[[167]].drop_duplicates().rename(columns={167:'txId'}).sort_values('txId').reset_index(drop=True)\n","df_nu = pd.concat([df_c_1, df_c_2]).drop_duplicates(subset=['txId'], keep='first')\n","#print(\"Nodos unicos: \", df_nu.shape)\n","\n","# 2. Obtenemos las clases de los nodos unicos\n","df_classes_smute = pd.merge(df_nu, X_res[[0, 'class']], how = 'inner', left_on=['txId'], right_on=[0]).drop([0], axis=1).drop_duplicates(subset=['txId'], keep='first').dropna(how='any').reset_index(drop=True)\n","print(\"df_classes_smute: \", df_classes_smute.shape)\n","print(\"Distribución df_classes_smute:\\n\", df_classes_smute['class'].value_counts())\n","\n","# 3. Obtener el complementario df_classes_smute para mas adelante. Todos los nodos creados pero que o bien tienen varias clases o bien no tienen porque son solo destinos\n","# pd.merge(df_classes_unknown, df_edges, indicator=True, how = 'outer', left_on=['txId'], right_on=['txId1']).query('_merge==\"right_only\"').drop(['txId', '_merge'], axis=1)\n","df_classes_smute_c = pd.merge(df_classes_smute['txId'], df_nu, indicator=True, how='outer', left_on=['txId'], right_on=['txId']).query('_merge==\"right_only\"'). drop(['_merge'], axis=1).reset_index(drop=True).drop_duplicates(subset=['txId'], keep='first')\n","print(\"Complementario df_classes_smute (solo nodos): \", df_classes_smute_c.shape)\n","# quitamos el nodo destino 0 que es \"la nada\"\n","\n","\n","df_classes_smute_c.head()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNLK2ojYjhlW"},"outputs":[],"source":["# 5.3 Reconstruimos df de features\n","df_features_smute = pd.merge(df_classes_smute['txId'], X_res.drop([167, 'class'], axis=1), how = 'inner', left_on=['txId'], right_on=[0]).drop(['txId'], axis=1).drop_duplicates(subset=[0], keep='first').reset_index(drop=True)\n","print(\"df_features_smute: \", df_features_smute.shape)\n","#df_features_smute.head()\n","\n","# Eliminamos del df_classes_smute aquellos nodos que no tengan features:\n","#df_classes_smute = pd.merge(df_classes_smute, df_features_smute[0], how='inner', left_on=['txId'], right_on=[0]).drop([0], axis=1).reset_index(drop=True)\n","print(\"df_classes_smute: \", df_classes_smute.shape)\n","#df_features_smute.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYyIDs37jaxj"},"outputs":[],"source":["# 5.4 Reconstruimos df de edges\n","\n","# VOY POR AQUIII\n","\n","df_edges_smute1 = pd.merge(df_classes_smute_c, X_res[[0, 167]], indicator=True, how = 'outer', left_on=['txId'], right_on=[0]).query('_merge==\"right_only\"').drop(['txId', '_merge'], axis=1)\n","print(\"df_edges_smute eliminando procesando origenes : \", df_edges_smute1.shape)\n","df_edges_smute2 = pd.merge(df_classes_smute_c, df_edges_smute1, indicator=True, how = 'outer', left_on=['txId'], right_on=[167]).query('_merge==\"right_only\"').drop(['txId', '_merge'], axis=1)\n","print(\"df_edges_smute procesando origen y destino\", df_edges_smute2.shape)\n","df_edges_smute2 = df_edges_smute2.rename(columns={0:'txId1', 167: 'txId2'}).astype(int).reset_index(drop=True)\n","df_edges_smute2.head()\n","\n","\n","# Montamos el df de edges\n","# , cogiendo solo los nodos de los datos sinteticos que tenian una unica clase\n","#df_e_1 = pd.merge(df_c['txId'], X_res, how = 'left', left_on=['txId'], right_on=[0]).drop([0], axis=1)\n","\n","# renombramos e imprimimos\n","#df_e = df_e_1[['txId', 167]].rename(columns={'txId':'txId1', 167:'txId2'})\n","#df_e = df_e.sort_values('txId1').reset_index(drop=True)\n","#print(df_e.shape)\n","#df_e.head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ya4mi92Bj76L"},"outputs":[],"source":["# ELIMINAR ESTO:Montamos df de features\n","# , cogiendo solo los nodos de los datos sinteticos que tenian una unica clase\n","#df_f_1 = pd.merge(df_c['txId'], X_res, how = 'inner', left_on=['txId'], right_on=[0]).drop([0], axis=1)\n","#df_f_2 = pd.merge(df_c['txId'], X_res, how = 'left', left_on=['txId'], right_on=[167]).drop([0], axis=1)\n","#df_f_3 = pd.concat([df_f_1, df_f_2]).drop_duplicates().dropna(how='any')\n","\n","#df_f = df_f_3.drop([167, 'class'], axis=1).rename(columns={'txId':0}).drop_duplicates().sort_values(0).reset_index(drop=True)\n","\n","#print(df_f.shape)\n","#df_f.head()\n","\n","#HASTA UNA DUDA: SALEN MÁS ELEMENOS EN df_f que en df_c y no puede ser. No entiendo porqué. Hacer un outer join para ver porqué .query('_merge==\"right_only\"')\n","#df_prueba = pd.merge(df_c, df_f, indicator=True, how = 'outer', left_on=['txId'], right_on=[0]).drop_duplicates()\n","#print(df_prueba['_merge'].value_counts())\n","#df_prueba.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pp7BswBKlmmf"},"outputs":[],"source":["# merging node features DF with classes DF\n","df_merge = df_features_smute.merge(df_classes_smute, how='inner', right_on=\"txId\", left_on=0)\n","df_merge = df_merge.sort_values(0).reset_index(drop=True)\n","\n","# extracting classified/non-classified nodes\n","classified = df_merge.loc[df_merge['class'].loc[df_merge['class']!=2].index].drop('txId', axis=1)\n","unclassified = df_merge.loc[df_merge['class'].loc[df_merge['class']==2].index].drop('txId', axis=1)\n","\n","# extracting classified/non-classified edges\n","classified_edges = df_edges_smute2.loc[df_edges_smute2['txId1'].isin(classified[0]) & df_edges_smute2['txId2'].isin(classified[0])]\n","unclassifed_edges = df_edges_smute2.loc[df_edges_smute2['txId1'].isin(unclassified[0]) | df_edges_smute2['txId2'].isin(unclassified[0])]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJLI5AvQlzIl"},"outputs":[],"source":["df_merge = df_merge.dropna()\n","df_merge = df_merge.reset_index(drop = True)\n","print(df_merge.shape)\n","df_merge.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMcka4QOITwZ"},"outputs":[],"source":["#PREPARING EDGES\n","# mapping nodes to indices\n","nodes = df_features_smute[0].values\n","map_id = {j:i for i,j in enumerate(nodes)}\n","\n","\n","# mapping edges to indices\n","edges = df_edges_smute2.copy()\n","edges.txId1 = edges.txId1.map(map_id)\n","edges.txId2 = edges.txId2.map(map_id)\n","edges = edges.dropna()\n","edges = edges.astype(int)\n","\n","\n","edge_index = np.array(edges.values).T\n","edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n","\n","\n","# weights for the edges are equal in case of model without attention\n","weights = torch.tensor([1] * edge_index.shape[1] , dtype=torch.float32)\n","\n","print(\"Total amount of edges in DAG:\", edge_index.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"34530f27"},"outputs":[],"source":["#PREPARING NODES\n","# maping node ids to corresponding indexes\n","node_features = df_merge.drop(['txId'], axis=1).copy()\n","node_features[0] = node_features[0].map(map_id)\n","\n","\n","classified_idx = node_features['class'].loc[node_features['class']!=2].index\n","unclassified_idx = node_features['class'].loc[node_features['class']==2].index\n","\n","# replace unkown class with 0, to avoid having 3 classes, this data/labels never used in training\n","node_features['class'] = node_features['class'].replace(2, 0) \n","\n","labels = node_features['class'].values\n","\n","# drop indeces, class and temporal axes \n","node_features = torch.tensor(np.array(node_features.drop([0, 'class', 1], axis=1).values, dtype=np.float32), dtype=torch.float32)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RLS2H9aEq0RG"},"outputs":[],"source":["#PyG DATASET\n","# converting data to PyGeometric graph data format\n","elliptic_dataset = Data(x = node_features, \n","                        edge_index = edge_index, \n","                        edge_attr = weights,\n","                        y = torch.tensor(labels, dtype=torch.float32)) \n","\n","print(f'Number of nodes: {elliptic_dataset.num_nodes}')\n","print(f'Number of node features: {elliptic_dataset.num_features}')\n","print(f'Number of edges: {elliptic_dataset.num_edges}')\n","print(f'Number of edge features: {elliptic_dataset.num_features}')\n","print(f'Average node degree: {elliptic_dataset.num_edges / elliptic_dataset.num_nodes:.2f}')\n","print(f'Number of classes: {len(np.unique(elliptic_dataset.y))}')\n","print(f'Has isolated nodes: {elliptic_dataset.has_isolated_nodes()}')\n","print(f'Has self loops: {elliptic_dataset.has_self_loops()}')\n","print(f'Is directed: {elliptic_dataset.is_directed()}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7nj9ZH3rENB"},"outputs":[],"source":["y_train = labels[classified_idx]\n","\n","# spliting train set and validation set\n","_, _, _, _, train_idx, valid_idx = \\\n","    train_test_split(node_features[classified_idx], \n","                     y_train, \n","                     classified_idx, \n","                     test_size=0.15, \n","                     random_state=Config.seed, \n","                     stratify=y_train)\n","                     \n","elliptic_dataset.train_idx = torch.tensor(train_idx, dtype=torch.long)\n","elliptic_dataset.val_idx = torch.tensor(valid_idx, dtype=torch.long)\n","elliptic_dataset.test_idx = torch.tensor(unclassified_idx, dtype=torch.long)\n","\n","print(\"Train dataset size:\", elliptic_dataset.train_idx.shape[0])\n","print(\"Validation dataset size:\", elliptic_dataset.val_idx.shape[0])\n","print(\"Test dataset size:\", elliptic_dataset.test_idx.shape[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-6ZfBQe-rHn_"},"outputs":[],"source":["#TRAIN/TEST HELPERS\n","def train_evaluate(model, data, criterion, optimizer, *args):\n","    num_epochs = args[0]\n","    checkpoints_dir = args[1]\n","    model_filename = args[2]\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    best_loss = 10e10\n","\n","    if not os.path.exists(checkpoints_dir):\n","        os.makedirs(checkpoints_dir)\n","\n","    model.train()\n","    for epoch in range(num_epochs+1):\n","        # Training\n","        optimizer.zero_grad()\n","        out = model(data.x, data.edge_index)\n","        loss = criterion(out[data.train_idx], data.y[data.train_idx].unsqueeze(1))\n","        acc = accuracy(out[data.train_idx], data.y[data.train_idx].unsqueeze(1), prediction_threshold=0.5)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        val_loss = criterion(out[data.val_idx], data.y[data.val_idx].unsqueeze(1))\n","        val_acc = accuracy(out[data.val_idx], data.y[data.val_idx].unsqueeze(1), prediction_threshold=0.5)\n","\n","        if(epoch % 10 == 0):\n","            #print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: '\n","            #      f'{acc*100:>6.2f}% | Val Loss: {val_loss:.2f} | '\n","            #      f'Val Acc: {val_acc*100:.2f}%')\n","        \n","            if val_loss < best_loss:\n","                best_loss = val_loss\n","                #print(\"Saving model for best loss\")\n","                checkpoint = {\n","                    'state_dict': best_model_wts\n","                }\n","                torch.save(checkpoint, os.path.join(checkpoints_dir, model_filename))\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","    return model\n","\n","def test(model, data):\n","    model.eval()\n","    out = model(data.x, data.edge_index) \n","    preds = ((torch.sigmoid(out) > 0.5).float()*1).squeeze(1)\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CsA18nmGrK4K"},"outputs":[],"source":["def accuracy(y_pred, y_test, prediction_threshold=0.5):\n","    y_pred_label = (torch.sigmoid(y_pred) > prediction_threshold).float()*1\n","\n","    correct_results_sum = (y_pred_label == y_test).sum().float()\n","    acc = correct_results_sum/y_test.shape[0]\n","\n","    return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ed4lr3irNsW"},"outputs":[],"source":["# Pruebas: General Model Class Implementation\n","import torch_geometric.nn.conv as conv\n","import inspect\n","class GNNGeneralModel(torch.nn.Module):\n","    \n","    def __init__(self, dim_in, dim_h, dim_out, model, num_layers=2):\n","        super(GNNGeneralModel, self).__init__()\n","\n","        self.num_layers = num_layers\n","        self.layers = nn.ModuleList()\n","        self.layers.append(conv.ClusterGCNConv(in_channels=dim_in, out_channels=dim_h))\n","\n","        if self.num_layers > 2:\n","          for i in range(1,self.num_layers-1,1):\n","              layer = GENConv(in_channels=dim_h, out_channels=dim_h) \n","              self.layers.append(layer)\n","\n","        layer = GATv2Conv(in_channels=dim_h, out_channels=dim_out)\n","        self.layers.append(layer)\n","\n","    def forward(self, x, edge_index):\n","\n","        for i in range(0,self.num_layers-1,1):\n","          h = self.layers[0](x, edge_index)\n","          h = torch.relu(h)\n","          h = F.dropout(h, p=0.6, training=self.training)\n","\n","\n","        out = self.layers[len(self.layers)-1](h, edge_index)\n","        return out\n","\n","    def get_num_layers(self):\n","      return len(self.layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xqNYeSNfrRsu"},"outputs":[],"source":["# Bucle funcionando\n","\n","import torch_geometric.nn.conv as conv\n","import inspect\n","#import pylibcugraphops\n","\n","# Obtenemos todas las clases del módulo conv\n","conv_classes = inspect.getmembers(conv, inspect.isclass)\n","\n","\n","# Seleccionamos sólo las clases que heredan de torch.nn.Module y tienen un método forward\n","conv_layers = [layer[1] for layer in conv_classes if issubclass(layer[1], torch.nn.Module) and \n","               hasattr(layer[1], 'forward') and \n","               'in_channels' in inspect.signature(layer[1].__init__).parameters and\n","               'out_channels' in inspect.signature(layer[1].__init__).parameters]\n","\n","conv_layers = [layer[1] for layer in conv_classes if issubclass(layer[1], torch.nn.Module) and \n","               hasattr(layer[1], 'forward') ]\n","\n","layers_not_used = []\n","reports = {}\n","fraud_pcnt = {} \n","# Creamos un modelo con cada capa convolucional en la lista\n","for conv_layer in conv_layers:\n","\n","    #params = inspect.signature(conv_layer.__init__).parameters\n","    #sig = inspect.signature(conv_layer.__init__)\n","    #required_params =  {name: param for name, param in sig.parameters.items() if param.default == inspect.Parameter.empty and name != \"self\" and name != \"kwargs\"}\n","    #required_params_set= [name for name in required_params.keys()]\n","\n","    try:\n","      #model = torch.nn.Sequential(\n","      #    conv_layer(in_channels=16, out_channels=32),\n","      #    conv_layer(in_channels=32, out_channels=64),\n","      #    conv_layer(in_channels=64, out_channels=128)\n","      #).to(Config.device)\n","      \n","      model = GNNGeneralModel(Config.input_dim, Config.hidden_size, Config.output_dim, conv_layer, Config.num_layers).to(Config.device)\n","      #print(model.get_num_layers())\n","      #print(model)\n","\n","      data_train = elliptic_dataset.to(Config.device)\n","\n","      optimizer = torch.optim.Adam(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n","      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","      criterion = torch.nn.BCEWithLogitsLoss()\n","      train_evaluate(model,\n","              data_train,\n","              criterion,\n","              optimizer,\n","              Config.num_epochs,\n","              Config.checkpoints_dir,\n","              type(conv_layer).__name__ + '.pth.tar')\n","      \n","      print(model)\n","      model.load_state_dict(torch.load(os.path.join(Config.checkpoints_dir, type(conv_layer).__name__ + '.pth.tar'))['state_dict'])\n","\n","      y_test_preds = test(model, data_train)\n","\n","      # confusion matrix on validation data\n","      #aux_conf_mat = data_train.y[data_train.val_idx].detach().cpu()\n","      #conf_mat = confusion_matrix(aux_conf_mat.numpy(), y_test_preds[valid_idx])\n","      conf_mat = confusion_matrix(data_train.y[data_train.val_idx].detach().cpu().numpy(), y_test_preds[valid_idx].cpu().numpy())\n","\n","      # impresión matriz de confusión\n","      plt.subplots(figsize=(6,6))\n","      sns.set(font_scale=1.4)\n","      sns.heatmap(conf_mat, annot=True, fmt=\".0f\", annot_kws={\"size\": 16}, cbar=False)\n","      plt.xlabel('Target (true) Class'); plt.ylabel('Output (predicted) class'); plt.title('Confusion Matrix')\n","      plt.show();\n","\n","      \n","      res = classification_report(data_train.y[data_train.val_idx].detach().cpu().numpy(),\n","                                  y_test_preds[valid_idx].cpu().numpy(),\n","                                  target_names=['licit', 'illicit'])\n","      print (res)\n","      reports[conv_layer] = res\n","\n","      print(f\"Test data fraud cases, percentage: {round (y_test_preds[data_train.test_idx].detach().cpu().numpy().sum() / len(data_train.y[data_train.test_idx]) *100, 2)} % \\n\")\n","      fraud_pcnt[conv_layer] = round(y_test_preds[data_train.test_idx].detach().cpu().numpy().sum() / len(data_train.y[data_train.test_idx]) *100, 2)\n","      \n","      \n","    except Exception as e:\n","      #print(f\"Error: {e}. Skipping item: {conv_layer}\")\n","      layers_not_used.append(conv_layer)\n","      continue\n","      \n","print(\"Finalizado.\\n\")\n","\n","\n","#ARMAConv, 0.86\n","#ClusterGCNConv, 0.93\n","#FeaStConv, 0.90\n","#FiLMConv, 0.87\n","#GATConv, 0.90\n","#GATv2Conv, 0.91\n","#GCNConv, 0.91\n","#GENConv, 0.92\n","#GeneralConv, 0.90\n","#GraphConv, 0.90\n","#HypergraphConv, 0.56\n","#LEConv, 0.90\n","#MFConv, 0.90\n","#ResGatedGraphConv, 0.90\n","#SAGEConv, 0.90\n","#SGConv, 0.90\n","#SuperGATConv, 0.91\n","#TAGConv, 0.90\n","#TransformerConv, 0.90\n","    "]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN+gI+3yQq1VN+n8QeV2pY2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}